{
    "CompletionsPiece": {
        "name": "CompletionsPiece",
        "dependency": {
            "dockerfile": null,
            "requirements_file": "requirements.txt"
        },
        "tags": [
            "OpenAI"
        ],
        "style": {
            "node_label": "OpenAI Completions",
            "node_type": "default",
            "node_style": {
                "backgroundColor": "#ebebeb"
            },
            "useIcon": true,
            "icon_class_name": "fa-regular fa-comment-dots",
            "iconStyle": {
                "cursor": "pointer"
            }
        },
        "description": "This Piece uses the OpenAI API to generate text completions.",
        "container_resources": {
            "requests": {
                "cpu": "100m",
                "memory": "128Mi"
            },
            "limits": {
                "cpu": "500m",
                "memory": "512Mi"
            }
        },
        "input_schema": {
            "title": "InputModel",
            "type": "object",
            "properties": {
                "llm_model_name": {
                    "description": "The name of the model to use. Options: ada, babbage, curie, davinci",
                    "allOf": [
                        {
                            "$ref": "#/definitions/LLMModelType"
                        }
                    ]
                },
                "prompt": {
                    "title": "Prompt",
                    "description": "The prompt to use for the completion",
                    "type": "string"
                },
                "max_tokens": {
                    "title": "Max Tokens",
                    "description": "The maximum number of tokens to generate in the completion.",
                    "default": 16,
                    "exclusiveMinimum": 0,
                    "type": "integer"
                },
                "temperature": {
                    "title": "Temperature",
                    "description": "What sampling temperature to use, between 0 and 2.",
                    "default": 1.0,
                    "exclusiveMinimum": 0.0,
                    "maximum": 2.0,
                    "type": "number"
                }
            },
            "required": [
                "llm_model_name",
                "prompt"
            ],
            "definitions": {
                "LLMModelType": {
                    "title": "LLMModelType",
                    "description": "An enumeration.",
                    "enum": [
                        "text-ada-001",
                        "text-babbage-001",
                        "text-curie-001",
                        "text-davinci-003"
                    ],
                    "type": "string"
                }
            }
        },
        "output_schema": {
            "title": "OutputModel",
            "type": "object",
            "properties": {
                "message": {
                    "title": "Message",
                    "description": "Output message to log",
                    "type": "string"
                },
                "result_text": {
                    "title": "Result Text",
                    "description": "The result of the text completion.",
                    "type": "string"
                },
                "usage_total_tokens": {
                    "title": "Usage Total Tokens",
                    "description": "The total number of tokens used in the completion.",
                    "type": "integer"
                }
            },
            "required": [
                "message",
                "result_text",
                "usage_total_tokens"
            ]
        },
        "secrets_schema": {
            "title": "SecretsModel",
            "type": "object",
            "properties": {
                "OPENAI_API_KEY": {
                    "title": "Openai Api Key",
                    "description": "OpenAI API key",
                    "type": "string"
                }
            },
            "required": [
                "OPENAI_API_KEY"
            ]
        }
    },
    "TextSummarizerPiece": {
        "name": "TextSummarizerPiece",
        "dependency": {
            "dockerfile": null,
            "requirements_file": "requirements.txt"
        },
        "tags": [],
        "style": {
            "node_label": "Piece",
            "node_type": "default",
            "node_style": {
                "backgroundColor": "#ebebeb"
            },
            "useIcon": true,
            "icon_class_name": "fa-solid fa-circle",
            "iconStyle": {
                "cursor": "pointer"
            }
        },
        "description": "This Piece runs a text summarizer using OpenAI API.",
        "input_schema": {
            "title": "InputModel",
            "description": "Text Summarizer Piece",
            "type": "object",
            "properties": {
                "text": {
                    "title": "Text",
                    "description": "Text to summarize",
                    "default": "",
                    "type": "string"
                },
                "openai_model": {
                    "description": "OpenAI model name to use for summarization",
                    "default": [
                        "gpt-3.5-turbo",
                        4000
                    ],
                    "allOf": [
                        {
                            "$ref": "#/definitions/LLMModelType"
                        }
                    ]
                },
                "chunk_size": {
                    "title": "Chunk Size",
                    "description": "Chunk size, measured in tokens, of each pre-summary chunk",
                    "default": 1000,
                    "type": "integer"
                },
                "chunk_overlap_rate": {
                    "title": "Chunk Overlap Rate",
                    "description": "The percentage of overlap between each chunk",
                    "default": 0.2,
                    "type": "number"
                },
                "max_tokens": {
                    "title": "Max Tokens",
                    "description": "The maximum number of tokens to generate in the summary.",
                    "default": 500,
                    "type": "integer"
                },
                "temperature": {
                    "title": "Temperature",
                    "description": "Temperature of the model, between 0 (more precise) and 1 (more creative)",
                    "default": 0.2,
                    "type": "number"
                }
            },
            "definitions": {
                "LLMModelType": {
                    "title": "LLMModelType",
                    "description": "An enumeration.",
                    "enum": [
                        [
                            "gpt-3.5-turbo",
                            4000
                        ],
                        [
                            "gpt-4",
                            8000
                        ]
                    ]
                }
            }
        },
        "output_schema": {
            "title": "OutputModel",
            "description": "Text Summarizer Piece",
            "type": "object",
            "properties": {
                "summarized_text": {
                    "title": "Summarized Text",
                    "description": "summarized text",
                    "type": "string"
                }
            },
            "required": [
                "summarized_text"
            ]
        },
        "secrets_schema": {
            "title": "SecretsModel",
            "description": "Text Summarizer Piece Secrets",
            "type": "object",
            "properties": {
                "OPENAI_API_KEY": {
                    "title": "Openai Api Key",
                    "description": "Your OpenAI API key",
                    "type": "string"
                }
            },
            "required": [
                "OPENAI_API_KEY"
            ]
        }
    },
    "AudioTranscriptionPiece": {
        "name": "AudioTranscriptionPiece",
        "dependency": {
            "dockerfile": "Dockerfile_AudioTranscription",
            "requirements_file": null
        },
        "tags": [
            "OpenAI"
        ],
        "style": {
            "node_label": "OpenAI Audio Transcript",
            "node_type": "default",
            "node_style": {
                "backgroundColor": "#ebebeb"
            },
            "useIcon": true,
            "icon_class_name": "fa-solid fa-headset",
            "iconStyle": {
                "cursor": "pointer"
            }
        },
        "description": "This Piece uses the OpenAI API to extract text transcripts from audio.",
        "container_resources": {
            "requests": {
                "cpu": "100m",
                "memory": "128Mi"
            },
            "limits": {
                "cpu": "500m",
                "memory": "512Mi"
            }
        },
        "input_schema": {
            "title": "InputModel",
            "type": "object",
            "properties": {
                "file_path": {
                    "title": "File Path",
                    "description": "The path to the text file to process.",
                    "type": "string"
                },
                "output_type": {
                    "description": "The type of output fot the result text.",
                    "default": "xcom",
                    "allOf": [
                        {
                            "$ref": "#/definitions/OutputTypeType"
                        }
                    ]
                },
                "prompt": {
                    "title": "Prompt",
                    "description": "An optional text to guide the model's style or continue a previous audio segment. The prompt should match the audio language.",
                    "type": "string"
                },
                "temperature": {
                    "title": "Temperature",
                    "description": "What sampling temperature to use, between 0 and 2.",
                    "default": 0.0,
                    "exclusiveMinimum": 0.0,
                    "maximum": 2.0,
                    "type": "number"
                }
            },
            "required": [
                "file_path",
                "prompt"
            ],
            "definitions": {
                "OutputTypeType": {
                    "title": "OutputTypeType",
                    "description": "An enumeration.",
                    "enum": [
                        "xcom",
                        "file"
                    ],
                    "type": "string"
                }
            }
        },
        "output_schema": {
            "title": "OutputModel",
            "type": "object",
            "properties": {
                "message": {
                    "title": "Message",
                    "description": "Output message to log",
                    "type": "string"
                },
                "transcription_result": {
                    "title": "Transcription Result",
                    "description": "The result transcription text.",
                    "default": "",
                    "type": "string"
                },
                "file_path": {
                    "title": "File Path",
                    "description": "The path to the results text file.",
                    "default": "",
                    "format": "file-path",
                    "type": "string"
                }
            },
            "required": [
                "message"
            ]
        },
        "secrets_schema": {
            "title": "SecretsModel",
            "type": "object",
            "properties": {
                "OPENAI_API_KEY": {
                    "title": "Openai Api Key",
                    "description": "OpenAI API key",
                    "type": "string"
                }
            },
            "required": [
                "OPENAI_API_KEY"
            ]
        }
    }
}